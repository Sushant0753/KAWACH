# -*- coding: utf-8 -*-
"""URLs Classification

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/urls-classification-8f044f24-47d0-4795-8782-6f25546b4675.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240216/auto/storage/goog4_request%26X-Goog-Date%3D20240216T172724Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D7c4d4f5b0274aab4e8b6ca02aa6cb16aaf859df5047ca03ae97e97d199c0b73e82cc518b2bbfca1e87d10a4d64e7bcdf2721e8374542c302fac59a5f4968744ae32e4540852032bbfbc31fa36a562f4d2649348c0c20e10271f42209a0d24cb8ce28c687a3c482d34d158e5caabcdf2cd2c927b056d22a3c8d2e83cec7ec5b5337d28f0f61f5fdc8a6b6e63718c0c14fa286dc6bcd808b3453b6f9620da2daeb6cfb8d0b5adf3dbf54a050aec8b12edc0cb577e9b3ccf9d3863c9bebcaa65ecd288b1d15b17c828162d430e51041998e37b08373133fb6beaad0d2b6f339f1a702fb2a562ab459ea033352fd91609bbec0028d61203e3da19cabb09eff7edda6
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil
import numpy as np
import pandas as pd
import re
from urllib.parse import urlparse
import os.path

dataset = pd.read_csv("datasets/malicious-url.csv")

dataset.head()

dataset.tail()

dataset.info()

print("Dataset shape:", dataset.shape)

dataset.keys()

dataset['type'].value_counts()

dataset['url'].value_counts()

n_samples, n_features = dataset.shape
print('Number of samples:', n_samples)
print('Number of features:', n_features)

dataset.isnull().sum()

print(dataset)

"""# PrePreprocessing"""

from sklearn.preprocessing import LabelEncoder

lb_make = LabelEncoder()
dataset["class_url"] = lb_make.fit_transform(dataset["type"])
dataset["class_url"].value_counts()

def url_length(url):
    return len(str(url))

dataset['url_length'] = dataset['url'].apply(lambda i: url_length(i))
dataset

from urllib.parse import urlparse

def hostname_length(url):
    return len(urlparse(url).netloc)

dataset['hostname_length'] = dataset['url'].apply(lambda i: hostname_length(i))
dataset

def count_www(url):
    url.count('www')
    return url.count('www')

dataset['count-www'] = dataset['url'].apply(lambda i: count_www(i))
dataset

def count_https(url):
    return url.count('https')

dataset['count-https'] = dataset['url'].apply(lambda i : count_https(i))

def count_http(url):
    return url.count('http')

dataset['count-http'] = dataset['url'].apply(lambda i : count_http(i))

def count_dot(url):
    count_dot = url.count('.')
    return count_dot

dataset['count.'] = dataset['url'].apply(lambda i: count_dot(i))

def count_per(url):
    return url.count('%')

dataset['count%'] = dataset['url'].apply(lambda i : count_per(i))

def count_ques(url):
    return url.count('?')

dataset['count?'] = dataset['url'].apply(lambda i: count_ques(i))

def count_hyphen(url):
    return url.count('-')

dataset['count-'] = dataset['url'].apply(lambda i: count_hyphen(i))

def count_equal(url):
    return url.count('=')

dataset['count='] = dataset['url'].apply(lambda i: count_equal(i))

def count_atrate(url):
    return url.count('@')

dataset['count@'] = dataset['url'].apply(lambda i: count_atrate(i))

def no_of_dir(url):
    urldir = urlparse(url).path
    return urldir.count('/')

dataset['count_dir'] = dataset['url'].apply(lambda i: no_of_dir(i))

def no_of_embed(url):
    urldir = urlparse(url).path
    return urldir.count('//')

dataset['count_embed_domian'] = dataset['url'].apply(lambda i: no_of_embed(i))
dataset

def shortening_service(url):
    match = re.search('bit\.ly|goo\.gl|shorte\.st|go2l\.ink|x\.co|ow\.ly|t\.co|tinyurl|tr\.im|is\.gd|cli\.gs|'
                      'yfrog\.com|migre\.me|ff\.im|tiny\.cc|url4\.eu|twit\.ac|su\.pr|twurl\.nl|snipurl\.com|'
                      'short\.to|BudURL\.com|ping\.fm|post\.ly|Just\.as|bkite\.com|snipr\.com|fic\.kr|loopt\.us|'
                      'doiop\.com|short\.ie|kl\.am|wp\.me|rubyurl\.com|om\.ly|to\.ly|bit\.do|t\.co|lnkd\.in|'
                      'db\.tt|qr\.ae|adataset\.ly|goo\.gl|bitly\.com|cur\.lv|tinyurl\.com|ow\.ly|bit\.ly|ity\.im|'
                      'q\.gs|is\.gd|po\.st|bc\.vc|twitthis\.com|u\.to|j\.mp|buzurl\.com|cutt\.us|u\.bb|yourls\.org|'
                      'x\.co|prettylinkpro\.com|scrnch\.me|filoops\.info|vzturl\.com|qr\.net|1url\.com|tweez\.me|v\.gd|'
                      'tr\.im|link\.zip\.net',
                      url)
    if match:
        return 1
    else:
        return 0

dataset['short_url'] = dataset['url'].apply(lambda i: shortening_service(i))


from tld import get_tld

#First Directory Length
def fd_length(url):
    urlpath= urlparse(url).path
    try:
        return len(urlpath.split('/')[1])
    except:
        return 0

dataset['fd_length'] = dataset['url'].apply(lambda i: fd_length(i))

#Length of Top Level Domain
dataset['tld'] = dataset['url'].apply(lambda i: get_tld(i,fail_silently=True))

def tld_length(tld):
    try:
        return len(tld)
    except:
        return -1

dataset['tld_length'] = dataset['tld'].apply(lambda i: tld_length(i))
dataset

def suspicious_words(url):
    match = re.search('PayPal|login|signin|bank|account|update|free|lucky|service|bonus|ebayisapi|webscr',
                      url)
    if match:
        return 1
    else:
        return 0

dataset['sus_url'] = dataset['url'].apply(lambda i: suspicious_words(i))


def digit_count(url):
    digits = 0
    for i in url:
        if i.isnumeric():
            digits = digits + 1
    return digits

dataset['count-digits']= dataset['url'].apply(lambda i: digit_count(i))


def letter_count(url):
    letters = 0
    for i in url:
        if i.isalpha():
            letters = letters + 1
    return letters


dataset['count-letters']= dataset['url'].apply(lambda i: letter_count(i))
dataset

def abnormal_url(url):
    hostname = urlparse(url).hostname
    hostname = str(hostname)
    match = re.search(hostname, url)
    if match:
        # print match.group()
        return 1
    else:
        # print 'No matching pattern found'
        return 0
dataset['abnormal_url'] = dataset['url'].apply(lambda i: abnormal_url(i))
dataset

#Use of IP or not in domain
def having_ip_address(url: str) -> int:
    match = re.search(
        '(([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.'
        '([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\/)|'  # IPv4
        '((0x[0-9a-fA-F]{1,2})\\.(0x[0-9a-fA-F]{1,2})\\.(0x[0-9a-fA-F]{1,2})\\.(0x[0-9a-fA-F]{1,2})\\/)' # IPv4 in hexadecimal
        '(?:[a-fA-F0-9]{1,4}:){7}[a-fA-F0-9]{1,4}', url)  # Ipv6
    if match:
        # print match.group()
        return 1
    else:
        # print 'No matching pattern found'
        return 0
dataset['use_of_ip_address'] = dataset['url'].apply(lambda i: having_ip_address(i))
dataset

dataset['use_of_ip_address'].value_counts()


from googlesearch import search
def google_index(url):
    site = search(url, 5)
    return 1 if site else 0
dataset['google_index'] = dataset['url'].apply(lambda i: google_index(i))
dataset

dataset.isnull().sum()

dataset.shape

"""# Model Training and Testing"""

X = dataset[['use_of_ip_address','abnormal_url', 'google_index', 'count-www', 'count@',
           'count_dir', 'count_embed_domian', 'short_url', 'count-https',
           'count-http', 'count%', 'count?', 'count-', 'count=', 'url_length',
           'hostname_length', 'sus_url', 'fd_length', 'tld_length', 'count-digits',
           'count-letters']]

y = dataset['class_url']

X

y

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, shuffle=True, random_state=5)

print(f"X_train Shape : {X_train.shape}")
print(f"Y_train Shape : {y_train.shape}")
print(f"X_test  Shape : {X_test.shape}")
print(f"Y_test  Shape : {y_test.shape}")

from sklearn.linear_model import LogisticRegression

from sklearn.metrics import confusion_matrix

from sklearn.metrics import accuracy_score



















from sklearn import metrics
from sklearn.ensemble import RandomForestClassifier
classifier5 = RandomForestClassifier(n_estimators=100,max_features='sqrt')
classifier5.fit(X_train.values,y_train.values)
y_pred5 = classifier5.predict(X_test.values)

cm5 = confusion_matrix(y_test, y_pred5)
print(cm5)

accuracy5 = metrics.accuracy_score(y_test, y_pred5)
print("Accuracy5:", accuracy5)

data = {
    'Model': ['RandomForestClassifier'],
    'Accuracy': [accuracy5]
}

accuracy_table = pd.DataFrame(data)
print(accuracy_table)




from joblib import dump

dump(classifier5, 'model.joblib')